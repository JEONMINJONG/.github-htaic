name: Release docker package

on:
  workflow_call:
    inputs:
      service_name:
        required: true
        description: "Name of helm controlled service to release"
        type: string
      ASSUME_ROLE:
        required: true
        description: "AWS arn to assume for execution"
        type: string
      working_dir:
        required: false
        description: "Operating path for path dependent steps"
        type: string
        default: .
      helm_values:
        required: false
        description: "Path of local helm values.yaml file"
        type: string
        default: DevValues.yaml
      cluster_name:
        required: false
        description: "Name of EKS cluster"
        type: string
        default: simplek8s
    secrets:
      GH_MANAGEPACKAGETOKEN:
        required: true
      APPID:
        required: true
      APPSECRET:
        required: true
      REGION:
        required: true

jobs:
  release:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.APPID }}
          aws-secret-access-key: ${{ secrets.APPSECRET }}
          aws-region: ${{ secrets.REGION }}
          #this piece isn't working, so I added an sts assume-role command in the next step
          # https://github.com/aws-actions/configure-aws-credentials/issues/465
          
      # get the micro front end helm chart for automatic deployment  
      - name: Checkout helm repo
        uses: actions/checkout@v3
        with:
          repository: htaic/.helm
          path: "${{ inputs.working_dir }}/.helm"
          ref: main
          token: "${{ secrets.GH_MANAGEPACKAGETOKEN }}"

      #https://stackoverflow.com/questions/63241009/aws-sts-assume-role-in-one-command
      # this assume role command ONLY WORKS if you do it at the begining of the step. If you do not include this in your step, the action's step is
      # not authenticated and will result in permission errors.
      - name: Authenticate and run Helm
        working-directory: ${{ inputs.working_dir }}
        run: |
          export $(printf "AWS_ACCESS_KEY_ID=%s AWS_SECRET_ACCESS_KEY=%s AWS_SESSION_TOKEN=%s" \
          $(aws sts assume-role \
          --role-arn $ASSUME_ROLE \
          --role-session-name GitHubPipeline \
          --query "Credentials.[AccessKeyId,SecretAccessKey,SessionToken]" \
          --output text))
          aws eks update-kubeconfig --region $AWS_REGION --name $K8S_NAME
          helm upgrade --install -f $VALUES_FILE $SERVICE_NAME ./.helm/microfrontend
          kubectl get all --all-namespaces
        env:
          ASSUME_ROLE: ${{ inputs.ASSUME_ROLE }}
          AWS_ACCESS_KEY_ID: ${{ secrets.APPID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.APPSECRET }}
          AWS_REGION: ${{ secrets.REGION }}
          K8S_NAME: ${{ inputs.cluster_name }}
          SERVICE_NAME: ${{ inputs.service_name }}
          VALUES_FILE: ${{ inputs.helm_values }}